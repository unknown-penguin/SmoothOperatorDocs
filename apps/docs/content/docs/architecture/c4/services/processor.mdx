---
title: Telemetry Processor Components
description: Component diagram for the Telemetry Processor showing internal components, error handling, and resilience patterns.
---

## Telemetry Processor & API — Components (C3)

The Telemetry Processor is the core processing engine that consumes raw telemetry, enriches it with context, detects anomalies, and persists data to the time-series database.

<Diagram lang="mermaid" chart={`
%%{init:{
  "theme":"base",
  "themeVariables": {
    "fontFamily":"Inter",
    "fontSize":"12px"
  },
  "config": {
    "flowchart": {
      "htmlLabels": true,
      "padding": 8,
      "nodeSpacing": 35,
      "rankSpacing": 35,
      "useMaxWidth": true
    }
  }
}}%%
flowchart LR

  subgraph Proc["Telemetry Processor & API (.NET)"]

    subgraph Consumers["Kafka Consumers"]
      ConsumerRaw["Raw Telemetry Consumer\ntelemetry.raw.v1\npartitioned by carId"]
      ConsumerCtx["Context Consumer\ntelemetry.context.v1"]
      DLQ["Dead Letter Queue Handler\npoison messages"]
    end

    subgraph Processing["Processing Pipeline"]
      Validator["Schema Validator\nJSON Schema"]
      Enricher["Enricher / Normalizer\ncontext merge, unit conversion,\nsignal filtering"]
      FailureDetector["Anomaly Detector\nrules + ML scoring"]
      Deduplicator["Deduplicator\nidempotency keys"]
    end

    subgraph Persistence["Persistence"]
      Persistor["Batch Writer -> TimescaleDB\nbatch insert, backoff retries"]
      CacheWriter["Cache Writer -> Redis\nstore latest, TTL 60s"]
    end

    subgraph Output["Output & APIs"]
      Producer["Kafka Producer\nenriched.v1 (idempotent)"]
      REST["REST API\n/query, /command (JWT)"]
      GRPC["gRPC API\nreal-time streaming"]
    end

    subgraph Resilience["Resilience & Monitoring"]
      CB["Circuit Breakers"]
      RL["Rate Limiter"]
      HC["Health Checks\n/health, /ready"]
      Metrics["Metrics Exporter\nPrometheus"]
      Logger["Structured Logger\nSerilog -> Loki"]
    end
  end

  %% External deps
  Kafka["Kafka Cluster"]:::infra
  TS["TimescaleDB"]:::infra
  Redis["Redis Cluster"]:::infra
  Meta["PostgreSQL (metadata)"]:::infra

  %% Consumption
  Kafka -->|"consume raw.v1"| ConsumerRaw
  Kafka -->|"consume context.v1"| ConsumerCtx
  ConsumerRaw --> Validator
  ConsumerCtx --> Validator
  Validator -->|"valid"| Enricher
  Validator -.->|"invalid"| DLQ

  %% Processing path
  Enricher --> Deduplicator
  Deduplicator --> FailureDetector
  Deduplicator -.->|"duplicate skipped"| Metrics

  %% Persistence guarded by CB
  FailureDetector --> Persistor
  Persistor -->|"CB protected"| CB
  CB --> TS
  Enricher --> CacheWriter
  CacheWriter -->|"CB protected"| CB
  CB --> Redis

  %% Metadata lookups
  Enricher -.->|"car metadata"| Meta
  FailureDetector -.->|"rules & thresholds"| Meta

  %% Output
  FailureDetector --> Producer
  Producer -->|"produce enriched.v1"| Kafka
  REST -->|"query (CB)"| CB
  GRPC -->|"stream (CB)"| CB
  CB --> TS
  REST -->|"cache read"| Redis
  REST --> RL
  GRPC --> RL

  %% Monitoring
  ConsumerRaw -.-> Metrics
  Persistor -.-> Metrics
  Producer -.-> Metrics
  REST -.-> Metrics
  CB -.-> Metrics

  Validator -.-> Logger
  Persistor -.-> Logger
  REST -.-> Logger

  %% Styles
  classDef infra fill:#374151,stroke:#111827,color:#ffffff;

`} />

### Key Architectural Decisions

**Resilience Patterns:**
- **Circuit Breakers**: Protect DB and Kafka from cascading failures (fail fast after threshold)
- **Dead Letter Queue**: Handle poison messages separately, retry with exponential backoff
- **Idempotent Processing**: Deduplicator ensures exactly-once semantics using message keys
- **Batch Writes**: Reduce DB load by batching 1000 records per write

**Error Handling:**
- **Schema Validation**: Reject malformed messages early, send to DLQ
- **Retry Logic**: 3 retries with exponential backoff for transient failures
- **Fallback Strategies**: Circuit open → log error, continue processing other messages
- **Graceful Degradation**: If cache unavailable, continue without caching (non-critical)

**Performance:**
- **Partitioned Consumption**: Kafka partitions by `carId` for parallel processing
- **Batch Processing**: 1000 records per DB batch insert
- **Connection Pooling**: DB connection pool maintained at >10% capacity
- **Caching Strategy**: Redis for latest values (60s TTL), avoid repeated DB queries

**Monitoring:**
- **Kafka Lag**: Alert if lag > 1000 messages
- **Write Throughput**: Track insertions/sec to TimescaleDB
- **Circuit Breaker State**: Log state transitions (closed → open)
- **API Rate Limits**: Track violations per user
